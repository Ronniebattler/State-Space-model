2.427,2.7149,2.7997,2.8511,2.6282,2.2586,2.4991,2.1768,1.902,
1.4846,1.461,1.8386,1.9981,1.8117,1.6863,1.8058,1.7261,1.7098,
1.7591,2.0231,2.2363,2.4441,2.2881,1.5004,0.3386,0.2233,0.7252,
1.0414,1.3161,1.4001,1.1876,1.1324,1.2918,1.3607,1.6618,2.6031,
4.0692,4.809,5.1876,5.1478,5.0717,5.2377,6.0564,6.6541,6.8811)
ninf <- length(inf)
plot(inf,type = 'l')
# y and its first lag
y0 <- inf[2:ninf]; y1 <- inf[1:(ninf-1)]
df <- data.frame(y = y0, y1 = y1)
#——————————————
# Single State : OLS
#——————————————
lrm <- lm(y~y1,df)
#——————————————
# Two State : Markov Switching (MSwM)
#——————————————
k  <- 2 # two-regime
mv <- 3 # means of 2 variables + 1 for volatility
rsm=msmFit(lrm, k=2, p=0, sw=rep(TRUE, mv),
control=list(parallel=FALSE))
graphics.off()  # clear all graphs
rm(list = ls()) # remove all files from your workspace
library(MSwM)
#——————————————
# Data : U.S. CPI inflation 12M, yoy
#——————————————
inf <- c(
3.6536,3.4686,2.9388,3.1676,3.5011,3.144,2.6851,2.6851,2.5591,
2.1053,1.8767,1.5909,1.1888,1.13,1.3537,1.6306,1.2332,1.0635,
1.455,1.7324,1.5046,2.0068,2.2285,2.45,2.7201,3.0976,2.9804,
2.1518,1.8764,1.93,2.0347,2.1919,2.3505,2.0214,1.91,2.0148,
2.006,1.6744,1.7251,2.2667,2.8566,3.1185,2.8972,2.5155,2.5075,
3.1411,3.5576,3.2877,2.8052,3.0073,3.1565,3.3065,2.8289,2.5093,
3.0211,3.582,4.6328,4.2581,3.284,3.284,3.9401,3.5736,3.3608,
3.5501,3.9002,4.0967,4.0227,3.8514,1.9921,1.3965,1.9496,2.4927,
2.0545,2.3914,2.7598,2.5599,2.6738,2.6571,2.2914,1.8797,2.7944,
3.547,4.2803,4.0266,4.205,4.0594,3.8979,3.8295,4.007,4.818,
5.3517,5.1719,4.8345,3.6631,1.0939,-0.0222,-0.1137,0.0085,-0.4475,
-0.578,-1.021,-1.2368,-1.9782,-1.495,-1.3875,-0.2242,1.8965,2.7753,
2.5873,2.1285,2.2604,2.1828,1.9837,1.1153,1.3319,1.1436,1.1121,
1.1599,1.0787,1.4276,1.6865,2.1026,2.5855,3.0308,3.4005,3.4424,
3.5173,3.6862,3.7417,3.4617,3.3932,3.0161,2.9644,2.857,2.5501,
2.2477,1.723,1.6403,1.4076,1.6719,1.931,2.1328,1.7801,1.7442,
1.67,1.998,1.5073,1.1324,1.3808,1.7012,1.8679,1.5271,1.0888,
0.873,1.2253,1.5015,1.5458,1.1142,1.5998,1.9951,2.1438,2.0381,
1.955,1.7006,1.67,1.5967,1.224,0.651,-0.2302,-0.0871,-0.022,
-0.1041,0.035,0.1794,0.2254,0.241,0.0088,0.1275,0.4354,0.6367,
1.2299,0.8437,0.8877,1.1658,1.0727,1.0735,0.8646,1.0498,1.5368,
1.6719,1.6703,2.0301,2.4802,2.7167,2.3602,2.186,1.866,1.6498,
1.7255,1.9187,2.2042,2.0132,2.1872,2.0797,2.0722,2.2013,2.3318,
2.427,2.7149,2.7997,2.8511,2.6282,2.2586,2.4991,2.1768,1.902,
1.4846,1.461,1.8386,1.9981,1.8117,1.6863,1.8058,1.7261,1.7098,
1.7591,2.0231,2.2363,2.4441,2.2881,1.5004,0.3386,0.2233,0.7252,
1.0414,1.3161,1.4001,1.1876,1.1324,1.2918,1.3607,1.6618,2.6031,
4.0692,4.809,5.1876,5.1478,5.0717,5.2377,6.0564,6.6541,6.8811)
ninf <- length(inf)
plot(inf,type = 'l')
# y and its first lag
y0 <- inf[2:ninf]; y1 <- inf[1:(ninf-1)]
df <- data.frame(y = y0, y1 = y1)
#——————————————
# Single State : OLS
#——————————————
lrm <- lm(y~y1,df)
#——————————————
# Two State : Markov Switching (MSwM)
#——————————————
k  <- 2 # two-regime
mv <- 3 # means of 2 variables + 1 for volatility
rsm=msmFit(lrm, k=2, p=0, sw=rep(TRUE, mv),
control=list(parallel=FALSE))
#——————————————
# function : Hamilton filter
#——————————————
# y = const0 + beta0*x + e0,
# y = const1 + beta1*x + e1,
#
# e0 ~ N(0,sigma0) : regime 1
# e1 ~ N(0,sigma1) : regime 2
##——————————————
hamilton.filter <- function(param,x,y){
# param = mle$par; x = y1; y = y0
nobs = length(y)
# constant, AR(1), sigma
const0 <- param[1]; const1 <- param[2]
beta0  <- param[3]; beta1  <- param[4]
sigma0 <- param[5]; sigma1 <- param[6]
# restrictions on range of probability (0~1)
p00 <- 1/(1+exp(-param[7]))
p11 <- 1/(1+exp(-param[8]))
p01 <- 1-p00; p10 <- 1-p11
# previous and updated state (fp)
ps_i0 <- us_j0 <- ps_i1 <- us_j1 <- NULL
# steady state probability
sspr1 <- (1-p00)/(2-p11-p00)
sspr0 <- 1-sspr1
# initial values for states (fp0)
us_j0 <- sspr0
us_j1 <- sspr1
# predicted state, filtered and smoothed probability
ps <- fp <- sp <- matrix(0,nrow = length(y),ncol = 2)
loglike <- 0
for(t in 1:nobs){
#) t-1 state (previous state)
# -> use previous updated state
ps_i0 <- us_j0
ps_i1 <- us_j1
# 2) state transition from i to j (state propagation)
# -> use time invariant transition matrix
# 3) densities under the two regimes at t
# (data observations and state dependent errors)
# regression error
er0 <- y[t] - const0 - beta0*x[t]
er1 <- y[t] - const1 - beta1*x[t]
eta_j0 <- (1/(sqrt(2*pi*sigma0^2)))*exp(-(er0^2)/(2*(sigma0^2)))
eta_j1 <- (1/(sqrt(2*pi*sigma1^2)))*exp(-(er1^2)/(2*(sigma1^2)))
#4) conditional density of the time t observation
# (combined likelihood with state being collapsed):
f_yt <- ps_i0*p00*eta_j0 + # 0 -> 0
ps_i0*p01*eta_j1 + # 0 -> 1
ps_i1*p10*eta_j0 + # 1 -> 0
ps_i1*p11*eta_j1   # 1 -> 1
# check for numerical instability
if( f_yt < 0 || is.na(f_yt)) {
loglike <- -100000000; break
}
# 5)6) updated states
us_j0 <- (ps_i0*p00*eta_j0+ps_i1*p10*eta_j0)/f_yt
us_j1 <- (ps_i0*p01*eta_j1+ps_i1*p11*eta_j1)/f_yt
ps[t,] <- c(ps_i0, ps_i0) # predicted states
fp[t,] <- c(us_j0, us_j1) # filtered probability
# 7)
loglike <- loglike + log(f_yt)
}
return(list(loglike = -loglike, fp = fp, ps = ps ))
}
# objective function for optimization
rs.est <- function(param,x,y){
return(hamilton.filter(param,x,y)$loglike)
}
#——————————————
# Run numerical optimization for estimation
#——————————————
# use linear regression estimates as initial guesses
init_guess <- c(lrm$coefficients[1], lrm$coefficients[1],
lrm$coefficients[2], lrm$coefficients[2],
summary(lrm)$sigma, summary(lrm)$sigma, 3, 2.9)
# multiple optimization to reassess convergence
mle <- optim(init_guess, rs.est,
control = list(maxit=50000, trace=2),
method=c('BFGS'), x = y1, y = y0)
mle <- optim(mle$par, rs.est,
control = list(maxit=50000, trace=2),
method=c('Nelder-Mead'), x = y1, y = y0)
#——————————————
# Report outputs
#——————————————
# get filtering output : fp, ps
hf <- hamilton.filter(mle$par,x = y1, y = y0)
# draw filtered probabilities of two states
x11(); par(mfrow=c(2,1))
matplot(cbind(rsm@Fit@filtProb[,2],hf$fp[,2]),
main = 'Regime 1', type='l', cex.main=1)
matplot(cbind(rsm@Fit@filtProb[,1],hf$fp[,1]),
main = 'Regime 2', type='l', cex.main=1)
# comparison of parameters
invisible(print('—-Comparison of two results—-'))
invisible(print('constant and beta1'))
cbind(rsm@Coef, cbind(mle$par[1:2],mle$par[3:4]))
invisible(print('sigma'))
cbind(rsm@std, mle$par[5:6])
invisible(print('p00, 011'))
cbind(diag(rsm@transMat), 1/(1+exp(-mle$par[7:8])))
invisible(print('loglikelihood'))
cbind(rsm@Fit@logLikel, mle$value)
#========================================================#
# Quantitative ALM, Financial Econometrics & Derivatives
# ML/DL using R, Python, Tensorflow by Sang-Heon Lee
#
# https://kiandlee.blogspot.com
#——————————————————#
# Lam’s generalized Hamilton model in Kim (1994)
#========================================================#
graphics.off()  # clear all graphs
rm(list = ls()) # remove all files from your workspace
#=======================================
#             Functions
#=======================================
# parameter constraints
trans = function(b0){
b1 = b0
# probability
b1[1:2] = exp(b0[1:2])/(1+exp(b0[1:2]))
# variance
b1[5] = b0[5]^2
# AR(1), AR(2) coefficients
XX6 = b0[6]/(1+abs(b0[6]))
XX7 = b0[7]/(1+abs(b0[7]))
b1[6] = XX6 + XX7
b1[7] = -1*XX6*XX7
return(b1)
}
# Kim filter = Hamilton + Kalman
loglike <- function(param_unc, y) {
# param_unc = init_param_unc; y = y
nobs <- length(y)
param <- trans(param_unc)
p11 <- param[1] # Pr[St=1/St-1=1]
p00 <- param[2] # Pr[St=0/St-1=0]
MU0 <- param[3]       # delta0
MU1 <- MU0 + param[4] # delta0 + delta1
VAR_W <- param[5]     # sigma^2
phi1 <- param[6]      # AR(1)
phi2 <- param[7]      # AR(2)
R <- matrix(0,1,1)
Q <- rbind(c(VAR_W, 0), c(0, 0))
H <- t(c(1, -1))
f <- rbind(c(phi1, phi2), c(1, 0))
# initialization
B_LL0 <- B_LL1 <- param[8:9] #c(0,0)
P_LL1 <- P_LL0 <-
matrix(solve(diag(4) - kronecker(f,f))%*%
as.vector(Q), nrow = 2)
# initialization with steady state probabilities
PROB_1 <- (1-p00)/(2-p11-p00) #Pr[S_0=1/Y_0]
PROB_0 <- 1-PROB_1            #Pr[S_0=0/Y_0]
#——————————————
#  START ITERATION
#——————————————
likv <- 0.0
for(t in 1:nobs) {
#———————————
# KALMAN FILTER
#———————————
# state prediction
B_TL00 <- f %*% B_LL0  # S=0, S’=0
B_TL01 <- f %*% B_LL0  # S=0, S’=1
B_TL10 <- f %*% B_LL1  # S=1, S’=0
B_TL11 <- f %*% B_LL1  # S=1, S’=1
# state prediction uncertainty
P_TL00 <- f %*% P_LL0 %*% t(f) + Q
P_TL01 <- f %*% P_LL0 %*% t(f) + Q
P_TL10 <- f %*% P_LL1 %*% t(f) + Q
P_TL11 <- f %*% P_LL1 %*% t(f) + Q
# forecast
fit00 <- H %*% B_TL00 + MU0
fit01 <- H %*% B_TL01 + MU1
fit10 <- H %*% B_TL10 + MU0
fit11 <- H %*% B_TL11 + MU1
# forecast error
F_CAST00 <- y[t] - fit00
F_CAST01 <- y[t] - fit01
F_CAST10 <- y[t] - fit10
F_CAST11 <- y[t] - fit11
# forecast error variance
SS00 <-  H %*% P_TL00 %*% t(H) + R
SS01 <-  H %*% P_TL01 %*% t(H) + R
SS10 <-  H %*% P_TL10 %*% t(H) + R
SS11 <-  H %*% P_TL11 %*% t(H) + R
# Kalman gain
kg00 <- P_TL00 %*% t(H) %*% solve(SS00)
kg01 <- P_TL01 %*% t(H) %*% solve(SS01)
kg10 <- P_TL10 %*% t(H) %*% solve(SS10)
kg11 <- P_TL11 %*% t(H) %*% solve(SS11)
# updating
B_TT00 <- B_TL00 + kg00 %*% F_CAST00
B_TT01 <- B_TL01 + kg01 %*% F_CAST01
B_TT10 <- B_TL10 + kg10 %*% F_CAST10
B_TT11 <- B_TL11 + kg11 %*% F_CAST11
P_TT00 <- (diag(2) - kg00 %*% H ) %*% P_TL00
P_TT01 <- (diag(2) - kg01 %*% H ) %*% P_TL01
P_TT10 <- (diag(2) - kg10 %*% H ) %*% P_TL10
P_TT11 <- (diag(2) - kg11 %*% H ) %*% P_TL11
#———————————
# HAMILTON FILTER
#———————————
# Pr[St,Yt/Yt-1]
NORM_PDF00 <- dnorm(F_CAST00,0,sqrt(SS00))
NORM_PDF01 <- dnorm(F_CAST01,0,sqrt(SS01))
NORM_PDF10 <- dnorm(F_CAST10,0,sqrt(SS10))
NORM_PDF11 <- dnorm(F_CAST11,0,sqrt(SS11))
PR_VL00 <- NORM_PDF00*(  p00)*PROB_0
PR_VL01 <- NORM_PDF01*(1-p00)*PROB_0
PR_VL10 <- NORM_PDF10*(1-p11)*PROB_1
PR_VL11 <- NORM_PDF11*(  p11)*PROB_1
# f(y_t|Y_{t-1})
PR_VAL <- PR_VL00 + PR_VL01 + PR_VL10 + PR_VL11
# Pr[St,St-1/Yt]
PRO_00 <- as.numeric(PR_VL00/PR_VAL)
PRO_01 <- as.numeric(PR_VL01/PR_VAL)
PRO_10 <- as.numeric(PR_VL10/PR_VAL)
PRO_11 <- as.numeric(PR_VL11/PR_VAL)
# Pr[St/Yt]
PROB_0 <- PRO_00 + PRO_10
PROB_1 <- PRO_01 + PRO_11
#———————————
# Kim’s Collapsing
#———————————
B_LL0 <- (PRO_00*B_TT00 + PRO_10*B_TT10)/PROB_0
B_LL1 <- (PRO_01*B_TT01 + PRO_11*B_TT11)/PROB_1
temp00 <- (B_LL0-B_TT00) %*% t(B_LL0-B_TT00)
temp10 <- (B_LL0-B_TT10) %*% t(B_LL0-B_TT10)
temp01 <- (B_LL1-B_TT01) %*% t(B_LL1-B_TT01)
temp11 <- (B_LL1-B_TT11) %*% t(B_LL1-B_TT11)
P_LL0 <- (PRO_00*(P_TT00 + temp00) +
PRO_10*(P_TT10 + temp10))/PROB_0
P_LL1 <- (PRO_01*(P_TT01 + temp01) +
PRO_11*(P_TT11 + temp11))/PROB_1
likv = likv -log(PR_VAL)
}
return(likv)
}
#=======================================
#                  Run
#=======================================
# Lam’s Real GNP Data Set : #1952.3 ~ 1984.4
rgnp <- c(  1378.2, 1406.8, 1431.4, 1444.9, 1438.2,
1426.6, 1406.8, 1401.2, 1418  , 1438.8, 1469.6,
1485.7, 1505.5, 1518.7, 1515.7, 1522.6, 1523.7,
1540.6, 1553.3, 1552.4, 1561.5, 1537.3, 1506.1,
1514.2, 1550  , 1586.7, 1606.4, 1637  , 1629.5,
1643.4, 1671.6, 1666.8, 1668.4, 1654.1, 1671.3,
1692.1, 1716.3, 1754.9, 1777.9, 1796.4, 1813.1,
1810.1, 1834.6, 1860  , 1892.5, 1906.1, 1948.7,
1965.4, 1985.2, 1993.7, 2036.9, 2066.4, 2099.3,
2147.6, 2190.1, 2195.8, 2218.3, 2229.2, 2241.8,
2255.2, 2287.7, 2300.6, 2327.3, 2366.9, 2385.3,
2383  , 2416.5, 2419.8, 2433.2, 2423.5, 2408.6,
2406.5, 2435.8, 2413.8, 2478.6, 2478.4, 2491.1,
2491  , 2545.6, 2595.1, 2622.1, 2671.3, 2734  ,
2741  , 2738.3, 2762.8, 2747.4, 2755.2, 2719.3,
2695.4, 2642.7, 2669.6, 2714.9, 2752.7, 2804.4,
2816.9, 2828.6, 2856.8, 2896  , 2942.7, 3001.8,
2994.1, 3020.5, 3115.9, 3142.6, 3181.6, 3181.7,
3178.7, 3207.4, 3201.3, 3233.4, 3157  , 3159.1,
3199.2, 3261.1, 3250.2, 3264.6, 3219  , 3170.4,
3179.9, 3154.5, 3159.3, 3186.6, 3258.3, 3306.4,
3365.1, 3444.7, 3487.1, 3507.4, 3520.4)
# # GNP growth rate : 1952.4 ~ 1984.4
y <- diff(log(rgnp),1)*100
# initial guess
init_param_unc <- c(3.5,0.0,-0.4,1.7,0.5,1,0.7,1,1)
# optimization
mle <- optim(init_param_unc, loglike, method=c('BFGS'),
control=list(maxit=50000,trace=2), y=y)
mle <- optim(mle$par, loglike, method=c('Nelder-Mead'),
control=list(maxit=50000,trace=2), y=y)
# compare estimation results with Kim (1994) paper
est_param <- cbind(c(trans(mle$par), -mle$value),
c(0.954, 0.456, -1.457, 2.421, 0.773,
1.246, -0.367, 5.224, 0.535, -176.33))
est_param[5,1] <- sqrt(est_param[5,1]) # var -> std
colnames(est_param) <- c('our est', 'Kim est')
rownames(est_param) <- c('p00','p11','delta0', 'delta1', 'sigma',
'phi1', 'phi2','x0','x-1','likv')
est_param
graphics.off()  # clear all graphs
rm(list = ls()) # remove all files from your workspace
library(MSwM)
#——————————————
# Data : U.S. CPI inflation 12M, yoy
#——————————————
inf <- c(
3.6536,3.4686,2.9388,3.1676,3.5011,3.144,2.6851,2.6851,2.5591,
2.1053,1.8767,1.5909,1.1888,1.13,1.3537,1.6306,1.2332,1.0635,
1.455,1.7324,1.5046,2.0068,2.2285,2.45,2.7201,3.0976,2.9804,
2.1518,1.8764,1.93,2.0347,2.1919,2.3505,2.0214,1.91,2.0148,
2.006,1.6744,1.7251,2.2667,2.8566,3.1185,2.8972,2.5155,2.5075,
3.1411,3.5576,3.2877,2.8052,3.0073,3.1565,3.3065,2.8289,2.5093,
3.0211,3.582,4.6328,4.2581,3.284,3.284,3.9401,3.5736,3.3608,
3.5501,3.9002,4.0967,4.0227,3.8514,1.9921,1.3965,1.9496,2.4927,
2.0545,2.3914,2.7598,2.5599,2.6738,2.6571,2.2914,1.8797,2.7944,
3.547,4.2803,4.0266,4.205,4.0594,3.8979,3.8295,4.007,4.818,
5.3517,5.1719,4.8345,3.6631,1.0939,-0.0222,-0.1137,0.0085,-0.4475,
-0.578,-1.021,-1.2368,-1.9782,-1.495,-1.3875,-0.2242,1.8965,2.7753,
2.5873,2.1285,2.2604,2.1828,1.9837,1.1153,1.3319,1.1436,1.1121,
1.1599,1.0787,1.4276,1.6865,2.1026,2.5855,3.0308,3.4005,3.4424,
3.5173,3.6862,3.7417,3.4617,3.3932,3.0161,2.9644,2.857,2.5501,
2.2477,1.723,1.6403,1.4076,1.6719,1.931,2.1328,1.7801,1.7442,
1.67,1.998,1.5073,1.1324,1.3808,1.7012,1.8679,1.5271,1.0888,
0.873,1.2253,1.5015,1.5458,1.1142,1.5998,1.9951,2.1438,2.0381,
1.955,1.7006,1.67,1.5967,1.224,0.651,-0.2302,-0.0871,-0.022,
-0.1041,0.035,0.1794,0.2254,0.241,0.0088,0.1275,0.4354,0.6367,
1.2299,0.8437,0.8877,1.1658,1.0727,1.0735,0.8646,1.0498,1.5368,
1.6719,1.6703,2.0301,2.4802,2.7167,2.3602,2.186,1.866,1.6498,
1.7255,1.9187,2.2042,2.0132,2.1872,2.0797,2.0722,2.2013,2.3318,
2.427,2.7149,2.7997,2.8511,2.6282,2.2586,2.4991,2.1768,1.902,
1.4846,1.461,1.8386,1.9981,1.8117,1.6863,1.8058,1.7261,1.7098,
1.7591,2.0231,2.2363,2.4441,2.2881,1.5004,0.3386,0.2233,0.7252,
1.0414,1.3161,1.4001,1.1876,1.1324,1.2918,1.3607,1.6618,2.6031,
4.0692,4.809,5.1876,5.1478,5.0717,5.2377,6.0564,6.6541,6.8811)
ninf <- length(inf)
plot(inf,type = 'l')
# y and its first lag
y0 <- inf[2:ninf]; y1 <- inf[1:(ninf-1)]
df <- data.frame(y = y0, y1 = y1)
#——————————————
# Single State : OLS
#——————————————
lrm <- lm(y~y1,df)
#——————————————
# Two State : Markov Switching (MSwM)
#——————————————
k  <- 2 # two-regime
mv <- 3 # means of 2 variables + 1 for volatility
rsm=msmFit(lrm, k=2, p=0, sw=rep(TRUE, mv),
control=list(parallel=FALSE))
#——————————————
# function : Hamilton filter
#——————————————
# y = const0 + beta0*x + e0,
# y = const1 + beta1*x + e1,
#
# e0 ~ N(0,sigma0) : regime 1
# e1 ~ N(0,sigma1) : regime 2
##——————————————
hamilton.filter <- function(param,x,y){
# param = mle$par; x = y1; y = y0
nobs = length(y)
# constant, AR(1), sigma
const0 <- param[1]; const1 <- param[2]
beta0  <- param[3]; beta1  <- param[4]
sigma0 <- param[5]; sigma1 <- param[6]
# restrictions on range of probability (0~1)
p00 <- 1/(1+exp(-param[7]))
p11 <- 1/(1+exp(-param[8]))
p01 <- 1-p00; p10 <- 1-p11
# previous and updated state (fp)
ps_i0 <- us_j0 <- ps_i1 <- us_j1 <- NULL
# steady state probability
sspr1 <- (1-p00)/(2-p11-p00)
sspr0 <- 1-sspr1
# initial values for states (fp0)
us_j0 <- sspr0
us_j1 <- sspr1
# predicted state, filtered and smoothed probability
ps <- fp <- sp <- matrix(0,nrow = length(y),ncol = 2)
loglike <- 0
for(t in 1:nobs){
#) t-1 state (previous state)
# -> use previous updated state
ps_i0 <- us_j0
ps_i1 <- us_j1
# 2) state transition from i to j (state propagation)
# -> use time invariant transition matrix
# 3) densities under the two regimes at t
# (data observations and state dependent errors)
# regression error
er0 <- y[t] - const0 - beta0*x[t]
er1 <- y[t] - const1 - beta1*x[t]
eta_j0 <- (1/(sqrt(2*pi*sigma0^2)))*exp(-(er0^2)/(2*(sigma0^2)))
eta_j1 <- (1/(sqrt(2*pi*sigma1^2)))*exp(-(er1^2)/(2*(sigma1^2)))
#4) conditional density of the time t observation
# (combined likelihood with state being collapsed):
f_yt <- ps_i0*p00*eta_j0 + # 0 -> 0
ps_i0*p01*eta_j1 + # 0 -> 1
ps_i1*p10*eta_j0 + # 1 -> 0
ps_i1*p11*eta_j1   # 1 -> 1
# check for numerical instability
if( f_yt < 0 || is.na(f_yt)) {
loglike <- -100000000; break
}
# 5)6) updated states
us_j0 <- (ps_i0*p00*eta_j0+ps_i1*p10*eta_j0)/f_yt
us_j1 <- (ps_i0*p01*eta_j1+ps_i1*p11*eta_j1)/f_yt
ps[t,] <- c(ps_i0, ps_i0) # predicted states
fp[t,] <- c(us_j0, us_j1) # filtered probability
# 7)
loglike <- loglike + log(f_yt)
}
return(list(loglike = -loglike, fp = fp, ps = ps ))
}
# objective function for optimization
rs.est <- function(param,x,y){
return(hamilton.filter(param,x,y)$loglike)
}
#——————————————
# Run numerical optimization for estimation
#——————————————
# use linear regression estimates as initial guesses
init_guess <- c(lrm$coefficients[1], lrm$coefficients[1],
lrm$coefficients[2], lrm$coefficients[2],
summary(lrm)$sigma, summary(lrm)$sigma, 3, 2.9)
# multiple optimization to reassess convergence
mle <- optim(init_guess, rs.est,
control = list(maxit=50000, trace=2),
method=c('BFGS'), x = y1, y = y0)
mle <- optim(mle$par, rs.est,
control = list(maxit=50000, trace=2),
method=c('Nelder-Mead'), x = y1, y = y0)
#——————————————
# Report outputs
#——————————————
# get filtering output : fp, ps
hf <- hamilton.filter(mle$par,x = y1, y = y0)
# draw filtered probabilities of two states
x11(); par(mfrow=c(2,1))
matplot(cbind(rsm@Fit@filtProb[,2],hf$fp[,2]),
main = 'Regime 1', type='l', cex.main=1)
matplot(cbind(rsm@Fit@filtProb[,1],hf$fp[,1]),
main = 'Regime 2', type='l', cex.main=1)
# comparison of parameters
invisible(print('—-Comparison of two results—-'))
invisible(print('constant and beta1'))
cbind(rsm@Coef, cbind(mle$par[1:2],mle$par[3:4]))
invisible(print('sigma'))
cbind(rsm@std, mle$par[5:6])
invisible(print('p00, 011'))
cbind(diag(rsm@transMat), 1/(1+exp(-mle$par[7:8])))
invisible(print('loglikelihood'))
cbind(rsm@Fit@logLikel, mle$value)
